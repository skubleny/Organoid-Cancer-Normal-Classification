---
title: "Machine learning models"
output: html_document
date: "2025-05-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Load required libraries
```{r}
library(glmnet)
library(caret)
library(pROC)
library(Metrics)
library(preprocessCore)
```

#Set up data
```{r}
#Add the "ground truth" label for ffpe and organoid tissues
normcancer_ffpe$type =  ifelse(grepl("67C", rownames(normcancer_ffpe), ignore.case = T), "Normal", ifelse(grepl("N", rownames(normcancer_ffpe), ignore.case = T), "Normal","Cancer"))

normcancer_organoid$type =  ifelse(grepl("67C", rownames(normcancer_organoid), ignore.case = T), "Normal", ifelse(grepl("N", rownames(normcancer_organoid), ignore.case = T), "Normal","Cancer"))

#Set up input data 
combined_edata = as.data.frame(combined_edata)

#Convert numeric columns to numeric
combined_edata[,2:ncol(combined_edata)] = lapply(2:ncol(combined_edata),function(x) as.numeric(combined_edata[[x]]))
```

#Model function 
```{r}
run_nested_cv_pipeline <- function(data, target_col = "type", seed = 123,
                                   external_validation = FALSE,
                                   external_datasets = list(ffpe = NULL, organoid = NULL),
                                   normalize_fn = quantileNormalizeByFeature,
                                   n_folds = 5, 
                                   lasso_nfolds = 10, 
                                   caret_summaryFunction = defaultSummary, 
                                   train_metric = "Accuracy",
                                   caret_trainctrl_cv = "repeatedcv"
                                   ) {

  set.seed(seed)
  folds <- createDataPartition(data[[target_col]], p = 0.8, list = TRUE, times = n_folds)

  all_results <- list()

  for (i in 1:n_folds) {
    train_idx <- folds[[i]]
    train_data <- data[train_idx, ]
    test_data <- data[-train_idx, ]

    x_train <- as.matrix(train_data[, !colnames(train_data) %in% target_col])
    y_train <- as.factor(train_data[[target_col]])
    x_test <- as.matrix(test_data[, !colnames(test_data) %in% target_col])
    y_test <- as.factor(test_data[[target_col]])

    # LASSO feature selection
    set.seed(seed)
    cv_lasso <- cv.glmnet(x_train, y_train, family = "multinomial", nfolds = lasso_nfolds)
    best_lambda <- cv_lasso$lambda.min
    
    #Extract features
    coef_lasso <- coef(cv_lasso, s = best_lambda)
    selected_features <- rownames(coef_lasso[[1]])
    selected_features <- selected_features[selected_features != "(Intercept)"]
    selected_features <- unique(selected_features[c(coef_lasso[[1]]@i,coef_lasso[[2]]@i)])
    
    #Make subset data with selected features
    x_train_selected <- x_train[, selected_features, drop = FALSE]
    x_test_selected <- x_test[, selected_features, drop = FALSE]

    #Train and test for nested models
    train_selected <- as.data.frame(x_train_selected)
    train_selected[[target_col]] <- y_train
    test_selected <- as.data.frame(x_test_selected)
    test_selected[[target_col]] <- y_test

    # Train caret models
    ctrl <- trainControl(method=caret_trainctrl_cv,number=5, repeats = 3, classProbs = TRUE, summaryFunction = 
                           caret_summaryFunction)

    set.seed(seed)
    model_glmnet <- train(as.formula(paste(target_col, "~ .")), data = train_selected,
                          method = "glmnet", family = "binomial", trControl = ctrl, metric = train_metric)

    set.seed(seed)
    model_rf <- train(as.formula(paste(target_col, "~ .")), data = train_selected,
                      method = "rf", trControl = ctrl, metric = train_metric)

    # Predict and evaluate internal
    pred_lasso <- as.factor(predict(cv_lasso, x_test, type="class"))
    pred_glmnet <- predict(model_glmnet, newdata = test_selected)
    pred_rf <- predict(model_rf, newdata = test_selected)

    prob_lasso <- as.data.frame(predict(cv_lasso, x_test, type="response")) %>% 
                                setNames(c(levels(pred_lasso)[1],levels(pred_lasso)[2])) #Need to make sure the colnames 
    #are consistent with the pred_names
    prob_glmnet <- predict(model_glmnet, newdata = test_selected, type = "prob")
    prob_rf <- predict(model_rf, newdata = test_selected, type = "prob")

    # Metrics function
    get_metrics <- function(truth, pred, prob) {
      cm <- confusionMatrix(pred, truth, positive = levels(truth)[1])
      auc_val <- roc(truth, prob[,1], levels=c("Normal", "Cancer"))$auc
      ci <- ci.auc(roc(truth, prob[,1],levels=c("Normal", "Cancer")))
      brier <- multiclass.Brier((prob)/rowSums(prob), truth)

      list(
        Accuracy = cm$overall["Accuracy"],
        Accuracy_lower = cm[["overall"]][["AccuracyLower"]],
        Accuracy_upper = cm[["overall"]][["AccuracyUpper"]],
        BalancedAccuracy = cm$byClass["Balanced Accuracy"],
        Kappa = cm$overall["Kappa"],
        Sensitivity = cm$byClass["Sensitivity"],
        Specificity = cm$byClass["Specificity"],
        AUC = auc_val,
        AUC_CI_Lower = ci[1],
        AUC_CI_Upper = ci[3],
        Brier = brier
      )
    }

    metrics_lasso <- get_metrics(y_test, pred_lasso, prob_lasso)
    metrics_glmnet <- get_metrics(y_test, pred_glmnet, prob_glmnet)
    metrics_rf <- get_metrics(y_test, pred_rf, prob_rf)

    # External validation
    ext_metrics <- list()
    if (external_validation) {
      for (ext_name in names(external_datasets)) {
        ext_data <- external_datasets[[ext_name]]
        ext_test <- as.factor(ext_data[[target_col]])
        
        #FSQN all features 
        ext_data <- as.matrix(ext_data[, !colnames(ext_data) %in% target_col])
             #Stop function if the target and test column names do not match.  
      if (!all(colnames(ext_data) %in% colnames(x_train))) {
        stop("colnames of `target` must match colnames of 'test'")
        }
        #set.seed(seed)
        ext_data_norm <- normalize_fn(ext_data, x_train)
        
        #FSQN selected features
        ext_data_selected <- ext_data[, selected_features, drop = FALSE]
             #Stop function if the target and test column names do not match.  
      if (!all(colnames(ext_data_selected) %in% colnames(x_train_selected))) {
        stop("colnames of `target` must match colnames of 'test'")
        }
        ext_data_norm_select <- normalize_fn(ext_data_selected, x_train_selected)

        ext_pred_lasso <- as.factor(predict(cv_lasso, ext_data_norm, type="class"))
        ext_prob_lasso <- as.data.frame(predict(cv_lasso, ext_data_norm, type="response")) %>% 
                                setNames(c(levels(ext_pred_lasso)[1],levels(ext_pred_lasso)[2])) #Need to make sure the 
        #colnames are consistent with the pred_names
        
        ext_pred_glmnet <- predict(model_glmnet, newdata = ext_data_norm_select)
        ext_prob_glmnet <- predict(model_glmnet, newdata = ext_data_norm_select, type = "prob")

        ext_pred_rf <- predict(model_rf, newdata = ext_data_norm_select)
        ext_prob_rf <- predict(model_rf, newdata = ext_data_norm_select, type = "prob")

        ext_metrics[[ext_name]] <- list(
          lasso = get_metrics(ext_test, ext_pred_lasso, ext_prob_lasso),
          glmnet = get_metrics(ext_test, ext_pred_glmnet, ext_prob_glmnet),
          rf = get_metrics(ext_test, ext_pred_rf, ext_prob_rf)
        )
      }
    }

    # Store results
    all_results[[paste0("Fold_", i)]] <- list(
      cv_glmnet_model = cv_lasso,
      selected_features = selected_features,
      caret_models = list(glmnet = model_glmnet, rf = model_rf),
      internal_metrics = list(lasso = metrics_lasso, glmnet = metrics_glmnet, rf = metrics_rf),
      external_metrics = ext_metrics
    )
  }

  return(all_results)
}

```

#Run it 
```{r}
results <- run_nested_cv_pipeline(
  data = combined_edata,
  target_col = "type",
  seed = 99,
  external_validation = TRUE,
  external_datasets = list(
    ffpe = normcancer_ffpe,
    organoid = normcancer_organoid
  ),
  normalize_fn = quantileNormalizeByFeature,
  n_folds = 5, 
  lasso_nfolds = 10,
  caret_summaryFunction = defaultSummary, 
  train_metric = "Accuracy",
  caret_trainctrl_cv = "repeatedcv"
)
```
#Compile it func
```{r}
export_cv_results_to_csv <- function(results_list, output_prefix = "cv_results") {
  all_metrics <- data.frame()
  
  for (fold_name in names(results_list)) {
    res <- results_list[[fold_name]]
    
    # Internal
    for (model_name in names(res$internal_metrics)) {
      metrics <- res$internal_metrics[[model_name]]
      metric_df <- data.frame(
        Fold = fold_name,
        Model = model_name,
        Dataset = "Internal",
        Metric = names(metrics),
        Value = as.numeric(metrics),
        stringsAsFactors = FALSE
      )
      all_metrics <- rbind(all_metrics, metric_df)
    }

    # External (if available)
    if (!is.null(res$external_metrics)) {
      for (ext_name in names(res$external_metrics)) {
        for (model_name in names(res$external_metrics[[ext_name]])) {
          metrics <- res$external_metrics[[ext_name]][[model_name]]
          metric_df <- data.frame(
            Fold = fold_name,
            Model = model_name,
            Dataset = ext_name,
            Metric = names(metrics),
            Value = as.numeric(metrics),
            stringsAsFactors = FALSE
          )
          all_metrics <- rbind(all_metrics, metric_df)
        }
      }
    }
  }

  # Save to CSV
  write.csv(all_metrics, paste0(output_prefix, "_longformat.csv"), row.names = FALSE)

  # Optional wide format
  wide_metrics <- reshape(all_metrics, idvar = c("Fold", "Model", "Dataset"),
                          timevar = "Metric", direction = "wide")
  write.csv(wide_metrics, paste0(output_prefix, "_wideformat.csv"), row.names = FALSE)

  return(list(long = all_metrics, wide = wide_metrics))
}

```

#Compile 
```{r}
metric_tables <- export_cv_results_to_csv(results, output_prefix = "nestedCV")
```



#Performance and analysis figures 

#AUC of selected model - get data for final model first
```{r}

data = combined_edata
  target_col = "type"
  seed = 99
  external_validation = TRUE
  external_datasets = list(
    ffpe = normcancer_ffpe,
    organoid = normcancer_organoid
  )
  normalize_fn = quantileNormalizeByFeature
  n_folds = 5
  lasso_nfolds = 10
  caret_summaryFunction = defaultSummary
  train_metric = "Accuracy"
  caret_trainctrl_cv = "repeatedcv"

  set.seed(seed)
  folds <- createDataPartition(data[[target_col]], p = 0.8, list = TRUE, times = n_folds)

 # all_results <- list()

  i = 1
    train_idx <- folds[[i]]
    train_data <- data[train_idx, ]
    test_data <- data[-train_idx, ]

    x_train <- as.matrix(train_data[, !colnames(train_data) %in% target_col])
    y_train <- as.factor(train_data[[target_col]])
    x_test <- as.matrix(test_data[, !colnames(test_data) %in% target_col])
    y_test <- as.factor(test_data[[target_col]])

    # LASSO feature selection
    set.seed(seed)
    cv_lasso <- cv.glmnet(x_train, y_train, family = "multinomial", nfolds = lasso_nfolds)
    best_lambda <- cv_lasso$lambda.min
    
    #Extract features
    coef_lasso <- coef(cv_lasso, s = best_lambda)
    selected_features <- rownames(coef_lasso[[1]])
    selected_features <- selected_features[selected_features != "(Intercept)"]
    selected_features <- unique(selected_features[c(coef_lasso[[1]]@i,coef_lasso[[2]]@i)])
    
    #Make subset data with selected features
    x_train_selected <- x_train[, selected_features, drop = FALSE]
    x_test_selected <- x_test[, selected_features, drop = FALSE]

    #Train and test for nested models
    train_selected <- as.data.frame(x_train_selected)
    train_selected[[target_col]] <- y_train
    test_selected <- as.data.frame(x_test_selected)
    test_selected[[target_col]] <- y_test

    # Train caret models
    ctrl <- trainControl(method=caret_trainctrl_cv,number=5, repeats = 3, classProbs = TRUE, summaryFunction = 
                           caret_summaryFunction)

    set.seed(seed)
    model_glmnet <- train(as.formula(paste(target_col, "~ .")), data = train_selected,
                          method = "glmnet", family = "binomial", trControl = ctrl, metric = train_metric)

    set.seed(seed)
    model_rf <- train(as.formula(paste(target_col, "~ .")), data = train_selected,
                      method = "rf", trControl = ctrl, metric = train_metric)

    # Predict and evaluate internal
    pred_lasso <- as.factor(predict(cv_lasso, x_test, type="class"))
    pred_glmnet <- predict(model_glmnet, newdata = test_selected)
    pred_rf <- predict(model_rf, newdata = test_selected)

    prob_lasso <- as.data.frame(predict(cv_lasso, x_test, type="response")) %>% 
                                setNames(c(levels(pred_lasso)[1],levels(pred_lasso)[2])) #Need to make sure the colnames 
    #are consistent with the pred_names
    prob_glmnet <- predict(model_glmnet, newdata = test_selected, type = "prob")
    prob_rf <- predict(model_rf, newdata = test_selected, type = "prob")

  

    # External validation ffpe
    #ext_metrics <- list()
    #if (external_validation) {
      #for (ext_name in names(external_datasets)) {
        ext_data <- external_datasets[["ffpe"]]
        ext_test <- as.factor(ext_data[[target_col]])
        
        #FSQN all features 
        ext_data <- as.matrix(ext_data[, !colnames(ext_data) %in% target_col])
             #Stop function if the target and test column names do not match.  
      if (!all(colnames(ext_data) %in% colnames(x_train))) {
        stop("colnames of `target` must match colnames of 'test'")
        }
        #set.seed(seed)
        ext_data_norm <- normalize_fn(ext_data, x_train)
        
        #FSQN selected features
        ext_data_selected <- ext_data[, selected_features, drop = FALSE]
             #Stop function if the target and test column names do not match.  
      if (!all(colnames(ext_data_selected) %in% colnames(x_train_selected))) {
        stop("colnames of `target` must match colnames of 'test'")
        }
        ext_data_norm_select <- normalize_fn(ext_data_selected, x_train_selected)

        ext_pred_lasso <- as.factor(predict(cv_lasso, ext_data_norm, type="class"))
        ext_prob_lasso <- as.data.frame(predict(cv_lasso, ext_data_norm, type="response")) %>% 
                                setNames(c(levels(ext_pred_lasso)[1],levels(ext_pred_lasso)[2])) #Need to make sure the 
        #colnames are consistent with the pred_names
        
        ext_pred_glmnet <- predict(model_glmnet, newdata = ext_data_norm_select)
        ext_prob_glmnet <- predict(model_glmnet, newdata = ext_data_norm_select, type = "prob")

        ext_pred_rf <- predict(model_rf, newdata = ext_data_norm_select)
        ext_prob_rf <- predict(model_rf, newdata = ext_data_norm_select, type = "prob")

```
#ROC FFPE 
```{r}

roc_glmfit_ffpe <- roc(ext_test, ext_prob_glmnet[,1], levels=c("Normal", "Cancer"))
auc(roc_glmfit_ffpe)
ci(roc_glmfit_ffpe)

ffpe_auc = ggroc(list(roc_glmfit_ffpe), linetype = 1, size = 1.4) + 
                xlab("Specificity") +
                ylab("Sensitivity") +
                ggtitle("ROC for FFPE Specimens") +
                geom_abline(intercept = 1, slope = 1, linetype = 2) +
                scale_colour_manual(name="Machine Learning Model",labels = c("AUC = 0.94 [95% CI: 0.86-1]"), values = c("#92C5DE")) +
                theme_classic() +
                theme(panel.grid.major.y = element_line(colour = "gray", size = 0.15)) +
                theme(panel.grid.minor.y = element_line(colour = "gray", size = 0.1)) +
                theme(panel.grid.major.x = element_line(colour = "gray", size = 0.15)) +
                theme(panel.grid.minor.x = element_line(colour = "gray", size = 0.1)) +
                theme(axis.text.x = element_text(colour="black", size = 12)) +
                theme(axis.text.y = element_text(colour="black",size = 12)) + 
                theme(plot.title = element_text(colour="black", size=12,hjust = 0, vjust=0)) +
                theme(axis.title.x = element_text(colour="black", size =12, vjust = 0.05)) +
                theme(axis.title.y = element_text(colour="black", size=12)) +
                theme(legend.title = element_text(color = "black", size = 12),
                      legend.text = element_text(color = "black", size = 12),
                      legend.position = c(0.55,0.17),
                      legend.background = element_rect(size=0.5, linetype="solid",colour ="black", fill=alpha("white",0.7))) + 
    guides(colour = guide_legend("Machine Learning Model"))

ggsave("ffpe_auc_new.svg", ffpe_auc, width=4, height=3.5)
ggsave("ffpe_auc_new.png", ffpe_auc, width=4, height=3.5)


```
#Get data for organoid selected model
```{r}
 # External validation ffpe
    #ext_metrics <- list()
    #if (external_validation) {
      #for (ext_name in names(external_datasets)) {
        ext_data <- external_datasets[["organoid"]]
        ext_test <- as.factor(ext_data[[target_col]])
        
        #FSQN all features 
        ext_data <- as.matrix(ext_data[, !colnames(ext_data) %in% target_col])
             #Stop function if the target and test column names do not match.  
      if (!all(colnames(ext_data) %in% colnames(x_train))) {
        stop("colnames of `target` must match colnames of 'test'")
        }
        #set.seed(seed)
        ext_data_norm <- normalize_fn(ext_data, x_train)
        
        #FSQN selected features
        ext_data_selected <- ext_data[, selected_features, drop = FALSE]
             #Stop function if the target and test column names do not match.  
      if (!all(colnames(ext_data_selected) %in% colnames(x_train_selected))) {
        stop("colnames of `target` must match colnames of 'test'")
        }
        ext_data_norm_select <- normalize_fn(ext_data_selected, x_train_selected)

        ext_pred_lasso <- as.factor(predict(cv_lasso, ext_data_norm, type="class"))
        ext_prob_lasso <- as.data.frame(predict(cv_lasso, ext_data_norm, type="response")) %>% 
                                setNames(c(levels(ext_pred_lasso)[1],levels(ext_pred_lasso)[2])) #Need to make sure the 
        #colnames are consistent with the pred_names
        
        ext_pred_glmnet <- predict(model_glmnet, newdata = ext_data_norm_select)
        ext_prob_glmnet <- predict(model_glmnet, newdata = ext_data_norm_select, type = "prob")

        ext_pred_rf <- predict(model_rf, newdata = ext_data_norm_select)
        ext_prob_rf <- predict(model_rf, newdata = ext_data_norm_select, type = "prob")

```
#ROC Organoid
```{r}
roc_glmfit_org <- roc(ext_test, ext_prob_glmnet[,1], levels=c("Normal", "Cancer"))
auc(roc_glmfit_org)
ci(roc_glmfit_org)


organoid_auc = ggroc(list(roc_glmfit_org), linetype = 1, size = 1.4) + 
                xlab("Specificity") +
                ylab("Sensitivity") +
                ggtitle("ROC for Organoid Specimens") +
                geom_abline(intercept = 1, slope = 1, linetype = 2) +
                scale_colour_manual(name="Machine Learning Model",labels = c("AUC = 0.85 [95% CI: 0.63-1]"), values = c("#EF8A62")) +
                theme_classic() +
                theme(panel.grid.major.y = element_line(colour = "gray", size = 0.15)) +
                theme(panel.grid.minor.y = element_line(colour = "gray", size = 0.1)) +
                theme(panel.grid.major.x = element_line(colour = "gray", size = 0.15)) +
                theme(panel.grid.minor.x = element_line(colour = "gray", size = 0.1)) +
                theme(axis.text.x = element_text(colour="black", size = 12)) +
                theme(axis.text.y = element_text(colour="black",size = 12)) + 
                theme(plot.title = element_text(colour="black", size=12,hjust = 0, vjust=0)) +
                theme(axis.title.x = element_text(colour="black", size =12, vjust = 0.05)) +
                theme(axis.title.y = element_text(colour="black", size=12)) +
                theme(legend.title = element_text(color = "black", size = 12),
                      legend.text = element_text(color = "black", size = 12),
                      legend.position = c(0.55,0.17),
                      legend.background = element_rect(size=0.5, linetype="solid",colour ="black", fill=alpha("white",0.7))) + 
    guides(colour = guide_legend("Machine Learning Model"))
        
ggsave("organoid_auc_new.svg", organoid_auc, width=4, height=3.5)
ggsave("organoid_auc_new.png", organoid_auc, width=4, height=3.5)

```
#Import accuracy data for training and testing data (same as metric_tables$long)
```{r}
urlfile<-'https://raw.githubusercontent.com/skubleny/Organoid-Cancer-Normal-Classification/main/Data/nestedCV_longformat.csv'
plot_df <- read.csv(urlfile)
```

#AUC plots 
```{r}
library(ggpubr)

plot_df$Model = as.factor(plot_df$Model)
plot_df$Dataset = as.factor(plot_df$Dataset)

levels(plot_df$Model)[levels(plot_df$Model)=="lasso"] <- "LASSO"
levels(plot_df$Model)[levels(plot_df$Model)=="glmnet"] <- "LASSO-EN"
levels(plot_df$Model)[levels(plot_df$Model)=="rf"] <- "LASSO-RF"
plot_df$Model <- factor(plot_df$Model, levels = c("LASSO", "LASSO-EN", "LASSO-RF"))

levels(plot_df$Dataset)[levels(plot_df$Dataset)=="Internal"] <- "ACRG"
levels(plot_df$Dataset)[levels(plot_df$Dataset)=="ffpe"] <- "FFPE"
levels(plot_df$Dataset)[levels(plot_df$Dataset)=="organoid"] <- "Organoid"
plot_df$Dataset <- factor(plot_df$Dataset, levels = c("ACRG", "FFPE", "Organoid"))


auc_combo = plot_df %>% dplyr::filter(Metric =="AUC") %>%
  ggbarplot(., x = "Dataset", y = "Value", fill = "Dataset", group = "Dataset", colour= "Dataset",facet.by = "Model",
            palette = "nejm", 
            add = "mean", 
            position = position_dodge(0.8),
            xlab = "Dataset",
            ylab = "AUC",
            title = "Model Performance",
            panel.labs.font = list(size = 14)) +
  stat_summary(fun.data = mean_cl_boot, geom='errorbar', color='black', width=0.2, linewidth=0.75) + 
      geom_pwc(aes(group = Dataset), 
               tip.length = 0.025,
               step.increase = 0.17, 
               method = "dunn_test", 
               p.adjust.method = "BH",
               label.size = 4.5,
               label = "p.adj.signif",
               bracket.nudge.y = 0.05)  + 
      scale_y_continuous(breaks = c(0, 0.5, 1),limits=c(0,1.25)) +
  font("xlab", size = 16, color = "black") +
  font("ylab", size = 16, color = "black") +
  font("x.text", size = 16, color = "black") + 
  font("title", size = 16, color = "black") +
  rremove("legend")

ggsave("auc_combo_plot.png", auc_combo, width=10, height=4)
ggsave("auc_combo_plot.svg", auc_combo, width=10, height=4)


#Values summarized in graph 

summary_auc = plot_df %>% dplyr::filter(Metric =="AUC") %>% group_by(Dataset, Model) %>% do(data.frame(rbind(Hmisc::smean.cl.boot(.$Value))))

write.csv(summary_auc, "summary_auc.csv")

```

#VarIMP Plots
```{r}
variable_importance = varImp(results$Fold_1$caret_models$glmnet) #Extract varImp from Fold 1 
variable_importance = variable_importance$importance

variable_importance = tibble::rownames_to_column(variable_importance, "Gene")


varimp_plot = ggplot(variable_importance, aes(x=Overall, y=reorder(Gene, +Overall), fill=Overall)) +
  geom_bar(stat="identity", colour="black", size = 1) +
  scale_fill_gradient(name = "Scaled Variable Importance",low = "#56B1F7", high = "#56B1F7", breaks=c(0,25,50,75,100), limits =c(0,100),labels=c(0,25,50,75,100)) +
  ylab("Covariate (Gene)") +
  xlab("Scaled ElasticNet Coefficient") +
  ggtitle("Variable importance:\nPrediction of cancer tissue") +
      theme_bw() +
  theme(axis.text.x = element_text(colour="black", size = 16)) +
  theme(axis.text.y = element_text(colour="black",size = 14, face = "italic")) + 
  theme(plot.title = element_text(colour="black", size=16,hjust = 0, vjust=0.5)) +
  theme(axis.title.x = element_text(colour="black", size =16, vjust =-1)) +
  theme(axis.title.y = element_text(colour="black", size=16)) +
  theme(legend.position = "none",
        legend.direction = "horizontal",
        legend.text =element_text(colour="black", size = 13),
        legend.title =element_text(colour="black", size = 13),
        legend.background = element_rect(size=0.3, linetype="solid",colour ="black", fill=alpha("white",0.7)))

ggsave("varimp_plot.svg", varimp_plot, width = 6, height=4.5)

```


#GSEA analysis 
```{r}
library(clusterProfiler)
library(msigdbr)
library(AnnotationDbi)
library(ggupset)
library(ggnewscale)
library(DOSE)
library(ggridges)
```
#Enricher for top multivariable factors 
```{r}
geneList <- variable_importance$Gene
```
```{r}
#Immunologic and Regulatory gene signatures
m_t2g <- msigdbr(species = "Homo sapiens", category = "C3") %>% dplyr::select(gs_name, gene_symbol)
m_t2g_2 <- msigdbr(species = "Homo sapiens", category = "C7") %>% dplyr::select(gs_name, gene_symbol)

m_t2g = rbind(m_t2g, m_t2g_2)
enrich_clust1 = enricher(geneList, 
                         pvalueCutoff  = 0.05,
                         pAdjustMethod = "BH",
                         TERM2GENE = m_t2g)
enrich_clust1
dotplot(enrich_clust1, showCategory=5) + ggtitle("Gene Cluster 1")
barplot(enrich_clust1, showCategory=5) 
heatplot(enrich_clust1, showCategory=5)
upsetplot(enrich_clust1)

#Remove mouse gene signatures 
enrich_gs = enrich_clust1
enrich_gs@result = enrich_gs@result[c(1,2,4),] #Remove mouse primary species
  

heatplot = heatplot(enrich_gs, showCategory=5)
heatplot = heatplot + theme(panel.border = element_rect(fill = NA,colour="black",size = 1, linetype = 1, linewidth = 1)) +
  ggtitle("Over Representation Analysis") +
  theme(axis.text.x = element_text(colour="black", size = 13, face="italic")) +
                theme(axis.text.y = element_text(colour="black",size = 13)) + 
                theme(plot.title = element_text(colour="black", size=15)) 
heatplot


ggsave("heatplot.svg", heatplot, width = 6, height=4.5)

```

