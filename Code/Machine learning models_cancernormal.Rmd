---
title: "Machine learning models"
output: html_document
date: "2025-05-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Load required libraries
```{r}
library(glmnet)
library(caret)
library(pROC)
library(Metrics)
library(preprocessCore)
```

#Set up data
```{r}
#Add the "ground truth" label for ffpe and organoid tissues
normcancer_ffpe$type =  ifelse(grepl("67C", rownames(normcancer_ffpe), ignore.case = T), "Normal", ifelse(grepl("N", rownames(normcancer_ffpe), ignore.case = T), "Normal","Cancer"))

normcancer_organoid$type =  ifelse(grepl("67C", rownames(normcancer_organoid), ignore.case = T), "Normal", ifelse(grepl("N", rownames(normcancer_organoid), ignore.case = T), "Normal","Cancer"))

#Set up input data 
combined_edata = as.data.frame(combined_edata)

#Convert numeric columns to numeric
combined_edata[,2:ncol(combined_edata)] = lapply(2:ncol(combined_edata),function(x) as.numeric(combined_edata[[x]]))
```

#Model function 
```{r}
run_nested_cv_pipeline <- function(data, target_col = "type", seed = 123,
                                   external_validation = FALSE,
                                   external_datasets = list(ffpe = NULL, organoid = NULL),
                                   normalize_fn = quantileNormalizeByFeature,
                                   n_folds = 5, 
                                   lasso_nfolds = 10, 
                                   caret_summaryFunction = defaultSummary, 
                                   train_metric = "Accuracy",
                                   caret_trainctrl_cv = "repeatedcv"
                                   ) {

  set.seed(seed)
  folds <- createDataPartition(data[[target_col]], p = 0.8, list = TRUE, times = n_folds)

  all_results <- list()

  for (i in 1:n_folds) {
    train_idx <- folds[[i]]
    train_data <- data[train_idx, ]
    test_data <- data[-train_idx, ]

    x_train <- as.matrix(train_data[, !colnames(train_data) %in% target_col])
    y_train <- as.factor(train_data[[target_col]])
    x_test <- as.matrix(test_data[, !colnames(test_data) %in% target_col])
    y_test <- as.factor(test_data[[target_col]])

    # LASSO feature selection
    set.seed(seed)
    cv_lasso <- cv.glmnet(x_train, y_train, family = "multinomial", nfolds = lasso_nfolds)
    best_lambda <- cv_lasso$lambda.min
    
    #Extract features
    coef_lasso <- coef(cv_lasso, s = best_lambda)
    selected_features <- rownames(coef_lasso[[1]])
    selected_features <- selected_features[selected_features != "(Intercept)"]
    selected_features <- unique(selected_features[c(coef_lasso[[1]]@i,coef_lasso[[2]]@i)])
    
    #Make subset data with selected features
    x_train_selected <- x_train[, selected_features, drop = FALSE]
    x_test_selected <- x_test[, selected_features, drop = FALSE]

    #Train and test for nested models
    train_selected <- as.data.frame(x_train_selected)
    train_selected[[target_col]] <- y_train
    test_selected <- as.data.frame(x_test_selected)
    test_selected[[target_col]] <- y_test

    # Train caret models
    ctrl <- trainControl(method=caret_trainctrl_cv,number=5, repeats = 3, classProbs = TRUE, summaryFunction = 
                           caret_summaryFunction)

    set.seed(seed)
    model_glmnet <- train(as.formula(paste(target_col, "~ .")), data = train_selected,
                          method = "glmnet", family = "binomial", trControl = ctrl, metric = train_metric)

    set.seed(seed)
    model_rf <- train(as.formula(paste(target_col, "~ .")), data = train_selected,
                      method = "rf", trControl = ctrl, metric = train_metric)

    # Predict and evaluate internal
    pred_lasso <- as.factor(predict(cv_lasso, x_test, type="class"))
    pred_glmnet <- predict(model_glmnet, newdata = test_selected)
    pred_rf <- predict(model_rf, newdata = test_selected)

    prob_lasso <- as.data.frame(predict(cv_lasso, x_test, type="response")) %>% 
                                setNames(c(levels(pred_lasso)[1],levels(pred_lasso)[2])) #Need to make sure the colnames 
    #are consistent with the pred_names
    prob_glmnet <- predict(model_glmnet, newdata = test_selected, type = "prob")
    prob_rf <- predict(model_rf, newdata = test_selected, type = "prob")

    # Metrics function
    get_metrics <- function(truth, pred, prob) {
      cm <- confusionMatrix(pred, truth, positive = levels(truth)[1])
      auc_val <- roc(truth, prob[,1], levels=c("Normal", "Cancer"))$auc
      ci <- ci.auc(roc(truth, prob[,1],levels=c("Normal", "Cancer")))
      brier <- multiclass.Brier((prob)/rowSums(prob), truth)

      list(
        Accuracy = cm$overall["Accuracy"],
        Accuracy_lower = cm[["overall"]][["AccuracyLower"]],
        Accuracy_upper = cm[["overall"]][["AccuracyUpper"]],
        BalancedAccuracy = cm$byClass["Balanced Accuracy"],
        Kappa = cm$overall["Kappa"],
        Sensitivity = cm$byClass["Sensitivity"],
        Specificity = cm$byClass["Specificity"],
        AUC = auc_val,
        AUC_CI_Lower = ci[1],
        AUC_CI_Upper = ci[3],
        Brier = brier
      )
    }

    metrics_lasso <- get_metrics(y_test, pred_lasso, prob_lasso)
    metrics_glmnet <- get_metrics(y_test, pred_glmnet, prob_glmnet)
    metrics_rf <- get_metrics(y_test, pred_rf, prob_rf)

    # External validation
    ext_metrics <- list()
    if (external_validation) {
      for (ext_name in names(external_datasets)) {
        ext_data <- external_datasets[[ext_name]]
        ext_test <- as.factor(ext_data[[target_col]])
        
        #FSQN all features 
        ext_data <- as.matrix(ext_data[, !colnames(ext_data) %in% target_col])
             #Stop function if the target and test column names do not match.  
      if (!all(colnames(ext_data) %in% colnames(x_train))) {
        stop("colnames of `target` must match colnames of 'test'")
        }
        #set.seed(seed)
        ext_data_norm <- normalize_fn(ext_data, x_train)
        
        #FSQN selected features
        ext_data_selected <- ext_data[, selected_features, drop = FALSE]
             #Stop function if the target and test column names do not match.  
      if (!all(colnames(ext_data_selected) %in% colnames(x_train_selected))) {
        stop("colnames of `target` must match colnames of 'test'")
        }
        ext_data_norm_select <- normalize_fn(ext_data_selected, x_train_selected)

        ext_pred_lasso <- as.factor(predict(cv_lasso, ext_data_norm, type="class"))
        ext_prob_lasso <- as.data.frame(predict(cv_lasso, ext_data_norm, type="response")) %>% 
                                setNames(c(levels(ext_pred_lasso)[1],levels(ext_pred_lasso)[2])) #Need to make sure the 
        #colnames are consistent with the pred_names
        
        ext_pred_glmnet <- predict(model_glmnet, newdata = ext_data_norm_select)
        ext_prob_glmnet <- predict(model_glmnet, newdata = ext_data_norm_select, type = "prob")

        ext_pred_rf <- predict(model_rf, newdata = ext_data_norm_select)
        ext_prob_rf <- predict(model_rf, newdata = ext_data_norm_select, type = "prob")

        ext_metrics[[ext_name]] <- list(
          lasso = get_metrics(ext_test, ext_pred_lasso, ext_prob_lasso),
          glmnet = get_metrics(ext_test, ext_pred_glmnet, ext_prob_glmnet),
          rf = get_metrics(ext_test, ext_pred_rf, ext_prob_rf)
        )
      }
    }

    # Store results
    all_results[[paste0("Fold_", i)]] <- list(
      cv_glmnet_model = cv_lasso,
      selected_features = selected_features,
      caret_models = list(glmnet = model_glmnet, rf = model_rf),
      internal_metrics = list(lasso = metrics_lasso, glmnet = metrics_glmnet, rf = metrics_rf),
      external_metrics = ext_metrics
    )
  }

  return(all_results)
}

```

#Run it 
```{r}
results <- run_nested_cv_pipeline(
  data = combined_edata,
  target_col = "type",
  seed = 99,
  external_validation = TRUE,
  external_datasets = list(
    ffpe = normcancer_ffpe,
    organoid = normcancer_organoid
  ),
  normalize_fn = quantileNormalizeByFeature,
  n_folds = 5, 
  lasso_nfolds = 10,
  caret_summaryFunction = defaultSummary, 
  train_metric = "Accuracy",
  caret_trainctrl_cv = "repeatedcv"
)
```
#Compile it func
```{r}
export_cv_results_to_csv <- function(results_list, output_prefix = "cv_results") {
  all_metrics <- data.frame()
  
  for (fold_name in names(results_list)) {
    res <- results_list[[fold_name]]
    
    # Internal
    for (model_name in names(res$internal_metrics)) {
      metrics <- res$internal_metrics[[model_name]]
      metric_df <- data.frame(
        Fold = fold_name,
        Model = model_name,
        Dataset = "Internal",
        Metric = names(metrics),
        Value = as.numeric(metrics),
        stringsAsFactors = FALSE
      )
      all_metrics <- rbind(all_metrics, metric_df)
    }

    # External (if available)
    if (!is.null(res$external_metrics)) {
      for (ext_name in names(res$external_metrics)) {
        for (model_name in names(res$external_metrics[[ext_name]])) {
          metrics <- res$external_metrics[[ext_name]][[model_name]]
          metric_df <- data.frame(
            Fold = fold_name,
            Model = model_name,
            Dataset = ext_name,
            Metric = names(metrics),
            Value = as.numeric(metrics),
            stringsAsFactors = FALSE
          )
          all_metrics <- rbind(all_metrics, metric_df)
        }
      }
    }
  }

  # Save to CSV
  write.csv(all_metrics, paste0(output_prefix, "_longformat.csv"), row.names = FALSE)

  # Optional wide format
  wide_metrics <- reshape(all_metrics, idvar = c("Fold", "Model", "Dataset"),
                          timevar = "Metric", direction = "wide")
  write.csv(wide_metrics, paste0(output_prefix, "_wideformat.csv"), row.names = FALSE)

  return(list(long = all_metrics, wide = wide_metrics))
}

```

#Compile 
```{r}
metric_tables <- export_cv_results_to_csv(results, output_prefix = "nestedCV")
```

